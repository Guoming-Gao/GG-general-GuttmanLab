{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.vectorized import contains\n",
    "from tifffile import imread\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "\n",
    "sns.set(color_codes=True, style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SPATIAL ANALYSIS PARAMETERS ===\n",
    "# Define the spatial scale and binning for pair correlation analysis\n",
    "\n",
    "nm_per_pxl = 23.4  # Nanometers per pixel (microscope calibration)\n",
    "r_max_nm = 1120  # Maximum distance to analyze (nm)\n",
    "ringwidth_nm = 100  # Width of each distance bin (nm)\n",
    "dr_slidingrings_nm = 20  # Step size between overlapping bins (nm)\n",
    "\n",
    "# === DISTANCE BIN SETUP ===\n",
    "# Create overlapping distance bins (sliding window approach)\n",
    "# This gives high spatial resolution by overlapping bins\n",
    "bins = np.arange(0, r_max_nm - ringwidth_nm, dr_slidingrings_nm)\n",
    "bin_starts = np.arange(0, r_max_nm - ringwidth_nm, dr_slidingrings_nm)\n",
    "bin_ends = bin_starts + ringwidth_nm\n",
    "\n",
    "# === PRECOMPUTE RING AREAS ===\n",
    "# Calculate ring areas for normalization (avoids repeated calculation)\n",
    "ring_areas_nm2 = np.pi * (bin_ends**2 - bin_starts**2)  # Ring areas in nm¬≤\n",
    "ring_areas_pxl2 = ring_areas_nm2 / (nm_per_pxl**2)  # Ring areas in pixel¬≤\n",
    "\n",
    "# === FILE PATH SETUP ===\n",
    "folder = \"/Volumes/guttman/Guoming_Gao-Resnick/Data_BIF/DEFAULT_USER/20250424_ONIdemo_Guoming/Guoming_data/dSTORM/dSTORM_tif/Malat1\"\n",
    "os.chdir(folder)\n",
    "\n",
    "# Data files (localization coordinates)\n",
    "ch1_file = \"dSTORM1_TX_nodiff_Malat_AF647_AF488-1-cropped-left-driftcorrected10k.csv\"\n",
    "ch2_file = \"dSTORM1_TX_nodiff_Malat_AF647_AF488-1-cropped-right-driftcorrected10k.csv\"\n",
    "\n",
    "# === LOAD DATA ONCE (MEMORY EFFICIENT) ===\n",
    "# Load both data files once and reuse for all cells\n",
    "# This avoids repeated file I/O operations\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "# Channel 1: Interest points (10% random sample)\n",
    "df_interest = pd.read_csv(\n",
    "    ch1_file,\n",
    "    skiprows=lambda i: i > 0 and random.random() > 0.1,  # Keep 10% of data\n",
    ")\n",
    "df_interest[\"x\"] = df_interest[\"x [nm]\"] / nm_per_pxl  # Convert to pixels\n",
    "df_interest[\"y\"] = df_interest[\"y [nm]\"] / nm_per_pxl\n",
    "\n",
    "# Channel 2: Reference points (100% of data)\n",
    "df_ref = pd.read_csv(\n",
    "    ch2_file,\n",
    "    skiprows=lambda i: i > 0 and random.random() > 1,  # Keep all data\n",
    ")\n",
    "df_ref[\"x\"] = df_ref[\"x [nm]\"] / nm_per_pxl  # Convert to pixels\n",
    "df_ref[\"y\"] = df_ref[\"y [nm]\"] / nm_per_pxl\n",
    "\n",
    "print(f\"Loaded {len(df_interest)} interest points and {len(df_ref)} reference points\")\n",
    "\n",
    "# === PARALLEL PROCESSING SETUP ===\n",
    "# Determine optimal number of CPU cores to use\n",
    "# Leave one core free for system operations\n",
    "# cell_roi_files = [\n",
    "#     f\n",
    "#     for f in os.listdir(\".\")\n",
    "#     if (f.startswith(ch1_file.split(\"-cropped\")[0]) & (\"cell\" in f))\n",
    "# ]\n",
    "# n_processes = min(len(cell_roi_files), mp.cpu_count() - 1)\n",
    "# print(f\"Using {n_processes} processes for {len(cell_roi_files)} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tifffile import imread\n",
    "from skimage.measure import find_contours\n",
    "from shapely.geometry import Polygon\n",
    "from matplotlib.patches import Polygon as MplPolygon\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMETERS TO TUNE\n",
    "# =============================================================================\n",
    "fname_befSTORM = \"dSTORM1_TX_nodiff_Malat_AF647_AF488-1_befbleach-composite-5x.tif\"\n",
    "\n",
    "# Gaussian smoothing parameters\n",
    "sigma = 30\n",
    "mode = \"reflect\"\n",
    "\n",
    "# Thresholding parameters\n",
    "threshold = 0.2  # Adjust this value (0.0 to 1.0 for normalized data)\n",
    "\n",
    "# Area filtering parameters\n",
    "area_threshold = 1e4  # Minimum area in pixels to keep objects\n",
    "# =============================================================================\n",
    "\n",
    "# Load and process your image\n",
    "img_befSTORM = imread(fname_befSTORM)\n",
    "img_befSTORM_ave = np.mean(img_befSTORM, axis=0)\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "img_smoothed = gaussian_filter(img_befSTORM_ave, sigma=sigma, mode=mode)\n",
    "\n",
    "# Create thresholded segmentation\n",
    "# Normalize the smoothed image to 0-1 range for consistent thresholding\n",
    "img_normalized = (img_smoothed - img_smoothed.min()) / (\n",
    "    img_smoothed.max() - img_smoothed.min()\n",
    ")\n",
    "img_thresholded = img_normalized > threshold\n",
    "\n",
    "# Extract contours using shapely\n",
    "contours = find_contours(img_thresholded, 0.5)\n",
    "\n",
    "# Convert contours to shapely polygons and filter by area\n",
    "filtered_polygons = []\n",
    "for contour in contours:\n",
    "    # Convert contour to polygon (note: contour is in row,col format)\n",
    "    if len(contour) >= 3:  # Need at least 3 points for a polygon\n",
    "        try:\n",
    "            polygon = Polygon(contour[:, [1, 0]])  # Switch to x,y format\n",
    "            if polygon.is_valid and polygon.area > area_threshold:\n",
    "                filtered_polygons.append(polygon)\n",
    "        except:\n",
    "            continue  # Skip invalid polygons\n",
    "\n",
    "print(\n",
    "    f\"Found {len(contours)} total contours, {len(filtered_polygons)} after area filtering\"\n",
    ")\n",
    "\n",
    "# Display all images with contours\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 7))\n",
    "\n",
    "axes[0].imshow(img_befSTORM_ave, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(img_smoothed, cmap=\"gray\")\n",
    "axes[1].set_title(f\"Smoothed (œÉ={sigma})\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(img_thresholded, cmap=\"gray\")\n",
    "axes[2].set_title(f\"Thresholded (t={threshold})\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Show original with filtered contours overlaid and indexed\n",
    "axes[3].imshow(img_befSTORM_ave, cmap=\"gray\")\n",
    "axes[3].set_title(f\"Contours (area>{area_threshold})\")\n",
    "for idx, polygon in enumerate(filtered_polygons):\n",
    "    # Draw contour\n",
    "    x, y = polygon.exterior.xy\n",
    "    axes[3].plot(x, y, \"r-\", linewidth=1.5)\n",
    "\n",
    "    # Add index label at polygon centroid\n",
    "    centroid = polygon.centroid\n",
    "    axes[3].text(\n",
    "        centroid.x,\n",
    "        centroid.y,\n",
    "        str(idx),\n",
    "        color=\"yellow\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "    )\n",
    "axes[3].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    ch1_file.split(\"-cropped\")[0] + \"-segmented_cell.png\",\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_processes = min(len(filtered_polygons), mp.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: PCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_within_cell_polygon_truly_vectorized(df, cell_polygon):\n",
    "    \"\"\"\n",
    "    Truly vectorized polygon filtering using shapely's vectorized operations.\n",
    "\n",
    "    Performance optimization strategy:\n",
    "    1. Quick bounds check (rectangular filter) - O(1) numpy operation\n",
    "    2. Detailed polygon check only on candidates - reduces expensive geometry ops\n",
    "    3. Use shapely's vectorized contains() - C-optimized batch processing\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'x' and 'y' columns\n",
    "        cell_polygon: Shapely Polygon object\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x_array, y_array) of points within polygon\n",
    "    \"\"\"\n",
    "    # Step 1: Fast rectangular bounds check (vectorized numpy operations)\n",
    "    # This eliminates most points without expensive geometric calculations\n",
    "    minx, miny, maxx, maxy = cell_polygon.bounds\n",
    "\n",
    "    mask_bounds = (\n",
    "        (df[\"x\"] >= minx) & (df[\"x\"] <= maxx) & (df[\"y\"] >= miny) & (df[\"y\"] <= maxy)\n",
    "    )\n",
    "\n",
    "    # Step 2: Only check detailed polygon containment for points within bounds\n",
    "    # This reduces the number of expensive Point().within() operations\n",
    "    df_candidates = df[mask_bounds]\n",
    "\n",
    "    if len(df_candidates) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Step 3: Use shapely's vectorized contains function (C-optimized)\n",
    "    # This is ~5-10x faster than creating Point objects in a loop\n",
    "    mask_within = contains(cell_polygon, df_candidates[\"x\"], df_candidates[\"y\"])\n",
    "\n",
    "    final_points = df_candidates[mask_within]\n",
    "    return final_points[\"x\"].values, final_points[\"y\"].values\n",
    "\n",
    "\n",
    "def quick_PCF_vectorized(\n",
    "    x_ref,\n",
    "    y_ref,\n",
    "    x_interest,\n",
    "    y_interest,\n",
    "    N_ref,\n",
    "    bin_starts,\n",
    "    bin_ends,\n",
    "    nm_per_pxl,\n",
    "    ring_areas_pxl2,\n",
    "    ring_areas_nm2,\n",
    "    cell_polygon,\n",
    "    rho_interest_per_nm2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Highly optimized PCF calculation with vectorization and efficient memory usage.\n",
    "\n",
    "    Key optimizations:\n",
    "    1. Vectorized distance calculations - all distances computed at once\n",
    "    2. Broadcasting for histogram binning - matrix operations instead of loops\n",
    "    3. Efficient edge correction calculation\n",
    "    4. Progress tracking for long computations\n",
    "\n",
    "    Args:\n",
    "        x_ref, y_ref: Reference point coordinates\n",
    "        x_interest, y_interest: Interest point coordinates\n",
    "        N_ref: Number of reference points\n",
    "        bin_starts, bin_ends: Distance bin boundaries\n",
    "        nm_per_pxl: Nanometers per pixel conversion\n",
    "        ring_areas_*: Precomputed ring areas for normalization\n",
    "        cell_polygon: Cell boundary for edge correction\n",
    "        rho_interest_per_nm2: Interest point density\n",
    "\n",
    "    Returns:\n",
    "        numpy.array: Pair correlation function values for each distance bin\n",
    "    \"\"\"\n",
    "\n",
    "    def process_reference_point(i):\n",
    "        \"\"\"\n",
    "        Process a single reference point with vectorized operations.\n",
    "\n",
    "        This function calculates:\n",
    "        1. Edge correction factors for this reference point\n",
    "        2. Distances to all interest points (vectorized)\n",
    "        3. Histogram binning (vectorized with broadcasting)\n",
    "        \"\"\"\n",
    "\n",
    "        # === EDGE CORRECTION CALCULATION ===\n",
    "        # Create annular rings around reference point for edge correction\n",
    "        # Each ring represents a distance bin\n",
    "        rings = [\n",
    "            Point(x_ref[i], y_ref[i])\n",
    "            .buffer(end)  # Create circle with radius = end\n",
    "            .difference(\n",
    "                Point(x_ref[i], y_ref[i]).buffer(start)\n",
    "            )  # Subtract inner circle\n",
    "            for start, end in zip(bin_starts / nm_per_pxl, bin_ends / nm_per_pxl)\n",
    "        ]\n",
    "\n",
    "        # Calculate how much of each ring intersects with cell boundary\n",
    "        # This determines the \"observable area\" for edge correction\n",
    "        intersect_areas = np.array(\n",
    "            [\n",
    "                cell_polygon.intersection(Polygon(ring), grid_size=0.1).area\n",
    "                for ring in rings\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Edge correction factor = 1 / (fraction of ring area observable)\n",
    "        # If only half the ring is in the cell, we need to multiply counts by 2\n",
    "        edge_correction_factors = 1 / (intersect_areas / ring_areas_pxl2)\n",
    "\n",
    "        # === VECTORIZED DISTANCE CALCULATION ===\n",
    "        # OLD WAY (slow): Loop through each interest point, calculate distance one by one\n",
    "        # NEW WAY (fast): Calculate ALL distances at once using numpy broadcasting\n",
    "\n",
    "        # Vectorized calculation: compute distances to ALL interest points simultaneously\n",
    "        # This replaces a loop of length N_interest with a single numpy operation\n",
    "        distances = (\n",
    "            np.sqrt((x_ref[i] - x_interest) ** 2 + (y_ref[i] - y_interest) ** 2)\n",
    "            * nm_per_pxl\n",
    "        )\n",
    "\n",
    "        # === VECTORIZED HISTOGRAM BINNING ===\n",
    "        # OLD WAY (slow): Loop through distances, check which bin each belongs to\n",
    "        # NEW WAY (fast): Use numpy broadcasting to create a matrix comparison\n",
    "\n",
    "        # Create boolean matrix: rows = bins, cols = distances\n",
    "        # hist_matrix[bin_idx, distance_idx] = True if distance falls in that bin\n",
    "        hist_matrix = (bin_starts[:, np.newaxis] <= distances) & (\n",
    "            bin_ends[:, np.newaxis] >= distances\n",
    "        )\n",
    "\n",
    "        # Sum across distances to get count per bin\n",
    "        hist_per_point = np.sum(hist_matrix, axis=1)\n",
    "\n",
    "        # Apply edge correction to compensate for boundary effects\n",
    "        return hist_per_point * edge_correction_factors\n",
    "\n",
    "    # === MAIN PCF CALCULATION ===\n",
    "    # Process each reference point with progress tracking\n",
    "    hist_results = []\n",
    "    for i in tqdm(range(len(x_ref)), desc=\"Calculating PCF\", leave=False):\n",
    "        hist_results.append(process_reference_point(i))\n",
    "\n",
    "    # === NORMALIZATION ===\n",
    "    # Convert raw counts to pair correlation function\n",
    "    # PCF = (observed pairs) / (expected pairs for random distribution)\n",
    "    norm_factors_cross = N_ref * ring_areas_nm2 * rho_interest_per_nm2\n",
    "\n",
    "    # Sum histograms from all reference points and normalize\n",
    "    PairCorr_cross = np.sum(hist_results, axis=0) / norm_factors_cross\n",
    "\n",
    "    return PairCorr_cross\n",
    "\n",
    "\n",
    "def create_cell_polygon(cell_roi_file, nm_per_pxl):\n",
    "    \"\"\"\n",
    "    Create a Shapely Polygon from ROI coordinate file.\n",
    "\n",
    "    Args:\n",
    "        cell_roi_file: Path to tab-separated file with polygon coordinates (in micrometers)\n",
    "        nm_per_pxl: Conversion factor from nanometers to pixels\n",
    "\n",
    "    Returns:\n",
    "        shapely.geometry.Polygon: Cell boundary polygon in pixel coordinates\n",
    "    \"\"\"\n",
    "    # Load coordinates (assumed to be in micrometers)\n",
    "    cell_outline_coordinates = pd.read_csv(cell_roi_file, sep=\"\\t\", header=None)\n",
    "\n",
    "    # Convert from micrometers to pixels: um * 1000 / nm_per_pxl\n",
    "    coords_roi = [\n",
    "        tuple(row * 1000 / nm_per_pxl) for _, row in cell_outline_coordinates.iterrows()\n",
    "    ]\n",
    "\n",
    "    cell_polygon = Polygon(coords_roi)\n",
    "    return cell_polygon\n",
    "\n",
    "\n",
    "def process_single_cell(args):\n",
    "    \"\"\"\n",
    "    Process a single cell ROI - optimized for multiprocessing.\n",
    "\n",
    "    This function is designed to be called in parallel for multiple cells.\n",
    "    All arguments are packed into a single tuple for multiprocessing compatibility.\n",
    "\n",
    "    Processing steps:\n",
    "    1. Create cell polygon from ROI file\n",
    "    2. Filter points to those within cell boundary (vectorized)\n",
    "    3. Calculate point densities\n",
    "    4. Compute pair correlation function\n",
    "    5. Return comprehensive results with metadata\n",
    "\n",
    "    Args:\n",
    "        args: Tuple containing all necessary parameters for processing\n",
    "\n",
    "    Returns:\n",
    "        dict: Results dictionary with PCF data and metadata, or error info\n",
    "    \"\"\"\n",
    "    (\n",
    "        # cell_roi_file,\n",
    "        cell_idx,\n",
    "        cell_polygon,\n",
    "        df_ref,\n",
    "        df_interest,\n",
    "        nm_per_pxl,\n",
    "        bin_starts,\n",
    "        bin_ends,\n",
    "        ring_areas_pxl2,\n",
    "        ring_areas_nm2,\n",
    "    ) = args\n",
    "\n",
    "    try:\n",
    "        # === CELL BOUNDARY SETUP ===\n",
    "        # cell_polygon = create_cell_polygon(cell_roi_file, nm_per_pxl)\n",
    "\n",
    "        # === POINT FILTERING (VECTORIZED) ===\n",
    "        # Filter points to only those within cell boundary using optimized vectorized method\n",
    "        x_ref, y_ref = corr_within_cell_polygon_truly_vectorized(df_ref, cell_polygon)\n",
    "        x_interest, y_interest = corr_within_cell_polygon_truly_vectorized(\n",
    "            df_interest, cell_polygon\n",
    "        )\n",
    "\n",
    "        # === VALIDATION ===\n",
    "        N_ref = x_ref.shape[0]\n",
    "        N_interest = x_interest.shape[0]\n",
    "\n",
    "        # Skip cells with no points (would cause division by zero)\n",
    "        if N_ref == 0 or N_interest == 0:\n",
    "            return {\n",
    "                # \"cell_name\": cell_roi_file,\n",
    "                \"error\": f\"No points found (N_ref={N_ref}, N_interest={N_interest})\",\n",
    "                \"success\": False,\n",
    "            }\n",
    "\n",
    "        # === DENSITY CALCULATIONS ===\n",
    "        # Calculate point densities for normalization\n",
    "        cell_polygon_area_nm2 = cell_polygon.area * (nm_per_pxl**2)\n",
    "        rho_ref_per_nm2 = N_ref / cell_polygon_area_nm2\n",
    "        rho_interest_per_nm2 = N_interest / cell_polygon_area_nm2\n",
    "\n",
    "        # === PCF CALCULATION ===\n",
    "        # Main computation using vectorized PCF function\n",
    "        pcf_cross = quick_PCF_vectorized(\n",
    "            x_ref,\n",
    "            y_ref,\n",
    "            x_interest,\n",
    "            y_interest,\n",
    "            N_ref,\n",
    "            bin_starts,\n",
    "            bin_ends,\n",
    "            nm_per_pxl,\n",
    "            ring_areas_pxl2,\n",
    "            ring_areas_nm2,\n",
    "            cell_polygon,\n",
    "            rho_interest_per_nm2,\n",
    "        )\n",
    "\n",
    "        # === RETURN COMPREHENSIVE RESULTS ===\n",
    "        return {\n",
    "            \"cell_name\": cell_idx,\n",
    "            \"pcf_cross\": pcf_cross,  # Main result: PCF values\n",
    "            \"N_ref\": N_ref,  # Reference point count\n",
    "            \"N_interest\": N_interest,  # Interest point count\n",
    "            \"rho_ref_per_nm2\": rho_ref_per_nm2,  # Reference point density\n",
    "            \"rho_interest_per_nm2\": rho_interest_per_nm2,  # Interest point density\n",
    "            \"cell_area_nm2\": cell_polygon_area_nm2,  # Cell area for validation\n",
    "            \"success\": True,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return error information for debugging\n",
    "        return {\"cell_name\": cell_idx, \"error\": str(e), \"success\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPARE PARALLEL PROCESSING ARGUMENTS ===\n",
    "# Package all arguments for each cell into tuples for multiprocessing\n",
    "process_args = [\n",
    "    (\n",
    "        # cell_roi_file,\n",
    "        cell_idx,\n",
    "        cell_polygon,\n",
    "        df_ref,\n",
    "        df_interest,\n",
    "        nm_per_pxl,\n",
    "        bin_starts,\n",
    "        bin_ends,\n",
    "        ring_areas_pxl2,\n",
    "        ring_areas_nm2,\n",
    "    )\n",
    "    # for cell_roi_file in cell_roi_files\n",
    "    for cell_idx, cell_polygon in enumerate(filtered_polygons)\n",
    "]\n",
    "\n",
    "# === PARALLEL PROCESSING EXECUTION ===\n",
    "# Use ThreadPoolExecutor instead of ProcessPoolExecutor for Jupyter compatibility\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "all_results = []  # Successful results\n",
    "failed_results = []  # Failed processing attempts\n",
    "\n",
    "print(\"Processing cells in parallel...\")\n",
    "with ThreadPoolExecutor(max_workers=n_processes) as executor:  # Changed this line\n",
    "    # === SUBMIT ALL JOBS ===\n",
    "    future_to_cell = {\n",
    "        executor.submit(process_single_cell, args): args[0] for args in process_args\n",
    "    }\n",
    "\n",
    "    # === COLLECT RESULTS WITH PROGRESS TRACKING ===\n",
    "    for future in tqdm(\n",
    "        as_completed(future_to_cell), total=len(future_to_cell), desc=\"Processing cells\"\n",
    "    ):\n",
    "        cell_name = future_to_cell[future]\n",
    "\n",
    "        try:\n",
    "            result = future.result()\n",
    "\n",
    "            if result[\"success\"]:\n",
    "                all_results.append(result)\n",
    "                print(\n",
    "                    f\"‚úì {result['cell_name']}: N_ref={result['N_ref']}, \"\n",
    "                    f\"N_interest={result['N_interest']}\"\n",
    "                )\n",
    "            else:\n",
    "                failed_results.append(result)\n",
    "                print(f\"‚úó {result['cell_name']}: {result['error']}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_results.append(\n",
    "                {\"cell_name\": cell_name, \"error\": str(e), \"success\": False}\n",
    "            )\n",
    "            print(f\"‚úó {cell_name}: {e}\")\n",
    "\n",
    "\n",
    "# === FINAL RESULTS SUMMARY ===\n",
    "# print(\n",
    "#     f\"\\nüéâ Successfully processed {len(all_results)} cells out of {len(cell_roi_files)} total\"\n",
    "# )\n",
    "print(\n",
    "    f\"\\nüéâ Successfully processed {len(all_results)} cells out of {len(filtered_polygons)} total\"\n",
    ")\n",
    "print(f\"‚ö†Ô∏è  Failed: {len(failed_results)} cells\")\n",
    "\n",
    "# === DETAILED RESULTS OUTPUT ===\n",
    "if all_results:\n",
    "    print(\"\\nüìä Detailed Results Summary:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Cell Name':<40} {'N_ref':<8} {'N_interest':<12} {'Area (nm¬≤)':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for result in all_results:\n",
    "        print(\n",
    "            f\"{result['cell_name']:<40} {result['N_ref']:<8} \"\n",
    "            f\"{result['N_interest']:<12} {result['cell_area_nm2']:<15.0f}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nüí° Access individual PCF results with: all_results[0]['pcf_cross']\")\n",
    "    print(\"üí° Distance bins available as: bin_starts and bin_ends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_quality_assessment(baseline_cv):\n",
    "    \"\"\"\n",
    "    Assess the quality of PCF normalization based on baseline coefficient of variation.\n",
    "    \"\"\"\n",
    "    if baseline_cv < 0.05:  # < 5%\n",
    "        return {\n",
    "            \"quality_category\": \"consistent_baseline\",\n",
    "            \"quality_description\": \"Very consistent baseline - reliable normalization\",\n",
    "            \"confidence\": \"high\",\n",
    "        }\n",
    "    elif baseline_cv < 0.10:  # 5-10%\n",
    "        return {\n",
    "            \"quality_category\": \"acceptable_noise_in_baseline\",\n",
    "            \"quality_description\": \"Acceptable noise level - normalization valid\",\n",
    "            \"confidence\": \"high\",\n",
    "        }\n",
    "    elif baseline_cv < 0.20:  # 10-20%\n",
    "        return {\n",
    "            \"quality_category\": \"some_variability_in_baseline\",\n",
    "            \"quality_description\": \"Some variability present - check data quality\",\n",
    "            \"confidence\": \"medium\",\n",
    "        }\n",
    "    elif baseline_cv < 0.30:  # 20-30%\n",
    "        return {\n",
    "            \"quality_category\": \"high_noise_in_baseline\",\n",
    "            \"quality_description\": \"High noise or residual correlations detected\",\n",
    "            \"confidence\": \"low\",\n",
    "        }\n",
    "    else:  # > 30%\n",
    "        return {\n",
    "            \"quality_category\": \"normalization_assumption_violated\",\n",
    "            \"quality_description\": \"Normalization assumption likely violated\",\n",
    "            \"confidence\": \"very_low\",\n",
    "        }\n",
    "\n",
    "\n",
    "def normalize_pcf_long_distance(\n",
    "    pcf_values,\n",
    "    n_bins_for_normalization=10,\n",
    "    bin_centers=None,\n",
    "    max_correlation_distance_nm=800,\n",
    "):\n",
    "    \"\"\"\n",
    "    Normalize PCF using long-distance bins to correct for systematic bias.\n",
    "    \"\"\"\n",
    "    # Ensure we have valid input\n",
    "    if len(pcf_values) < n_bins_for_normalization:\n",
    "        raise ValueError(\n",
    "            f\"PCF has {len(pcf_values)} bins, need at least {n_bins_for_normalization}\"\n",
    "        )\n",
    "\n",
    "    # Method 1: Use last N bins regardless of distance\n",
    "    normalization_bins = pcf_values[-n_bins_for_normalization:]\n",
    "\n",
    "    # Method 2: If bin centers provided, only use bins beyond max_correlation_distance\n",
    "    if bin_centers is not None:\n",
    "        long_distance_mask = bin_centers >= max_correlation_distance_nm\n",
    "        if np.sum(long_distance_mask) >= n_bins_for_normalization:\n",
    "            normalization_bins = pcf_values[long_distance_mask]\n",
    "\n",
    "    # Calculate normalization factor and statistics\n",
    "    normalization_factor = np.mean(normalization_bins)\n",
    "    normalization_std = np.std(normalization_bins)\n",
    "    baseline_coefficient_of_variation = normalization_std / normalization_factor\n",
    "\n",
    "    # Apply normalization\n",
    "    normalized_pcf = pcf_values / normalization_factor\n",
    "\n",
    "    # Assess quality based on baseline CV\n",
    "    quality_assessment = get_baseline_quality_assessment(\n",
    "        baseline_coefficient_of_variation\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"normalized_pcf\": normalized_pcf,\n",
    "        \"raw_pcf\": pcf_values,\n",
    "        \"normalization_factor\": normalization_factor,\n",
    "        \"normalization_bins\": normalization_bins,\n",
    "        \"normalization_std\": normalization_std,\n",
    "        \"baseline_cv\": baseline_coefficient_of_variation,\n",
    "        \"n_bins_used\": len(normalization_bins),\n",
    "        \"quality_assessment\": quality_assessment,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_all_pcf_results(\n",
    "    all_results,\n",
    "    bin_starts,\n",
    "    bin_ends,\n",
    "    n_bins_for_normalization=10,\n",
    "    max_correlation_distance_nm=800,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply long-distance normalization to all PCF results with quality assessment.\n",
    "    \"\"\"\n",
    "    # Calculate bin centers for distance-based filtering\n",
    "    bin_centers = (bin_starts + bin_ends) / 2\n",
    "\n",
    "    print(\n",
    "        f\"Normalizing {len(all_results)} cells | Distance range: {bin_centers[0]:.0f}-{bin_centers[-1]:.0f} nm\"\n",
    "    )\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Cell Name':<40} {'Norm Factor':<12} {'Baseline CV':<12} {'Quality':<25}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    normalized_results = []\n",
    "\n",
    "    for result in tqdm(all_results, desc=\"Normalizing PCF\", leave=False):\n",
    "        try:\n",
    "            # Apply normalization\n",
    "            norm_result = normalize_pcf_long_distance(\n",
    "                result[\"pcf_cross\"],\n",
    "                n_bins_for_normalization=n_bins_for_normalization,\n",
    "                bin_centers=bin_centers,\n",
    "                max_correlation_distance_nm=max_correlation_distance_nm,\n",
    "            )\n",
    "\n",
    "            # Create updated result dictionary\n",
    "            updated_result = result.copy()\n",
    "            updated_result.update(\n",
    "                {\n",
    "                    \"pcf_normalized\": norm_result[\"normalized_pcf\"],\n",
    "                    \"pcf_raw\": norm_result[\"raw_pcf\"],\n",
    "                    \"normalization_factor\": norm_result[\"normalization_factor\"],\n",
    "                    \"normalization_std\": norm_result[\"normalization_std\"],\n",
    "                    \"baseline_cv\": norm_result[\"baseline_cv\"],\n",
    "                    \"n_normalization_bins\": norm_result[\"n_bins_used\"],\n",
    "                    \"quality_assessment\": norm_result[\"quality_assessment\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            normalized_results.append(updated_result)\n",
    "\n",
    "            # Get quality info for display\n",
    "            quality = norm_result[\"quality_assessment\"]\n",
    "            status = (\n",
    "                \"‚úÖ\"\n",
    "                if quality[\"confidence\"] == \"high\"\n",
    "                else \"üü°\" if quality[\"confidence\"] == \"medium\" else \"‚ùå\"\n",
    "            )\n",
    "\n",
    "            # Print concise results\n",
    "            # cell_name_short = (\n",
    "            #     result[\"cell_name\"][:38] + \"..\"\n",
    "            #     if len(result[\"cell_name\"]) > 40\n",
    "            #     else result[\"cell_name\"]\n",
    "            # )\n",
    "            print(\n",
    "                f\"{status} {cell_name:<40} {norm_result['normalization_factor']:<12.3f} \"\n",
    "                f\"{norm_result['baseline_cv']:<12.1%} {quality['quality_category'].replace('_', ' '):<25}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error normalizing {result['cell_name']}: {e}\")\n",
    "            normalized_results.append(result)\n",
    "\n",
    "    return normalized_results\n",
    "\n",
    "\n",
    "def analyze_normalization_quality(normalized_results):\n",
    "    \"\"\"\n",
    "    Analyze the quality and consistency of normalization across cells.\n",
    "    \"\"\"\n",
    "    # Extract normalization data\n",
    "    norm_factors = []\n",
    "    baseline_cvs = []\n",
    "    quality_categories = []\n",
    "\n",
    "    for r in normalized_results:\n",
    "        if \"normalization_factor\" in r and not np.isnan(\n",
    "            r.get(\"normalization_factor\", np.nan)\n",
    "        ):\n",
    "            norm_factors.append(r[\"normalization_factor\"])\n",
    "            baseline_cvs.append(r.get(\"baseline_cv\", np.nan))\n",
    "            quality_categories.append(\n",
    "                r.get(\"quality_assessment\", {}).get(\"quality_category\", \"unknown\")\n",
    "            )\n",
    "\n",
    "    if len(norm_factors) == 0:\n",
    "        return {\"error\": \"No valid normalization factors found\"}\n",
    "\n",
    "    # Convert to arrays\n",
    "    norm_factors = np.array(norm_factors)\n",
    "    baseline_cvs = np.array([cv for cv in baseline_cvs if not np.isnan(cv)])\n",
    "\n",
    "    # Count quality categories\n",
    "    from collections import Counter\n",
    "\n",
    "    quality_counts = Counter(quality_categories)\n",
    "\n",
    "    analysis = {\n",
    "        \"n_cells_normalized\": len(norm_factors),\n",
    "        \"mean_normalization_factor\": np.mean(norm_factors),\n",
    "        \"std_normalization_factor\": np.std(norm_factors),\n",
    "        \"cv_normalization_factors\": np.std(norm_factors) / np.mean(norm_factors),\n",
    "        \"median_baseline_cv\": np.median(baseline_cvs),\n",
    "        \"mean_baseline_cv\": np.mean(baseline_cvs),\n",
    "        \"quality_breakdown\": dict(quality_counts),\n",
    "        \"high_confidence_fraction\": (\n",
    "            quality_counts.get(\"consistent_baseline\", 0)\n",
    "            + quality_counts.get(\"acceptable_noise_in_baseline\", 0)\n",
    "        )\n",
    "        / len(norm_factors),\n",
    "    }\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply long-distance normalization to results\n",
    "if \"all_results\" not in locals() or len(all_results) == 0:\n",
    "    print(\"‚ö†Ô∏è  No PCF results found. Please run the main PCF calculation first.\")\n",
    "else:\n",
    "    normalized_results = process_all_pcf_results(\n",
    "        all_results,\n",
    "        bin_starts,\n",
    "        bin_ends,\n",
    "        n_bins_for_normalization=10,\n",
    "        max_correlation_distance_nm=800,\n",
    "    )\n",
    "\n",
    "# Analyze normalization quality\n",
    "norm_analysis = analyze_normalization_quality(normalized_results)\n",
    "\n",
    "if \"error\" not in norm_analysis:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä NORMALIZATION QUALITY SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"Cells processed: {norm_analysis['n_cells_normalized']}\")\n",
    "    print(\n",
    "        f\"Mean normalization factor: {norm_analysis['mean_normalization_factor']:.3f} ¬± {norm_analysis['std_normalization_factor']:.3f}\"\n",
    "    )\n",
    "    print(f\"Baseline noise (median CV): {norm_analysis['median_baseline_cv']:.1%}\")\n",
    "    print(f\"High confidence results: {norm_analysis['high_confidence_fraction']:.1%}\")\n",
    "\n",
    "    # Quality breakdown\n",
    "    print(f\"\\nüìã Quality Breakdown:\")\n",
    "    quality_breakdown = norm_analysis[\"quality_breakdown\"]\n",
    "    total_cells = sum(quality_breakdown.values())\n",
    "\n",
    "    for category, count in quality_breakdown.items():\n",
    "        percentage = count / total_cells * 100\n",
    "        category_display = category.replace(\"_\", \" \").title()\n",
    "        icon = (\n",
    "            \"‚úÖ\"\n",
    "            if category in [\"consistent_baseline\", \"acceptable_noise_in_baseline\"]\n",
    "            else \"üü°\" if category == \"some_variability_in_baseline\" else \"‚ùå\"\n",
    "        )\n",
    "        print(f\"   {icon} {category_display}: {count} cells ({percentage:.1f}%)\")\n",
    "\n",
    "    # Interpretation\n",
    "    print(f\"\\nüí° Key Findings:\")\n",
    "    mean_factor = norm_analysis[\"mean_normalization_factor\"]\n",
    "    if abs(mean_factor - 1.0) > 0.1:\n",
    "        bias_type = \"over-estimation\" if mean_factor > 1.1 else \"under-estimation\"\n",
    "        print(f\"   ‚Ä¢ Systematic {bias_type} detected (factor = {mean_factor:.2f})\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Good baseline calibration (factor ‚âà 1.0)\")\n",
    "\n",
    "    cv_factors = norm_analysis[\"cv_normalization_factors\"]\n",
    "    consistency = \"high variability\" if cv_factors > 0.2 else \"good consistency\"\n",
    "    print(f\"   ‚Ä¢ {consistency.title()} between cells (CV = {cv_factors:.1%})\")\n",
    "\n",
    "    high_conf_fraction = norm_analysis[\"high_confidence_fraction\"]\n",
    "    if high_conf_fraction > 0.8:\n",
    "        conf_level = \"High overall confidence\"\n",
    "    elif high_conf_fraction > 0.6:\n",
    "        conf_level = \"Moderate confidence\"\n",
    "    else:\n",
    "        conf_level = \"Low confidence - investigate data quality\"\n",
    "    print(f\"   ‚Ä¢ {conf_level} ({high_conf_fraction:.1%} reliable)\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå {norm_analysis['error']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Access results: normalized_results[i]['pcf_normalized']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Prepare comprehensive data package for saving\n",
    "def save_pcf_results(\n",
    "    normalized_results,\n",
    "    bin_starts,\n",
    "    bin_ends,\n",
    "    nm_per_pxl,\n",
    "    r_max_nm,\n",
    "    ringwidth_nm,\n",
    "    dr_slidingrings_nm,\n",
    "    ch1_file,\n",
    "    ch2_file,\n",
    "    prefix,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save PCF results with complete metadata to pickle file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate bin centers for convenience\n",
    "    bin_centers = (bin_starts + bin_ends) / 2\n",
    "\n",
    "    # Create comprehensive data package\n",
    "    pcf_data_package = {\n",
    "        # === MAIN RESULTS ===\n",
    "        \"normalized_results\": normalized_results,\n",
    "        \"n_cells\": len(normalized_results),\n",
    "        # === SPATIAL PARAMETERS ===\n",
    "        \"nm_per_pxl\": nm_per_pxl,\n",
    "        \"r_max_nm\": r_max_nm,\n",
    "        \"ringwidth_nm\": ringwidth_nm,\n",
    "        \"dr_slidingrings_nm\": dr_slidingrings_nm,\n",
    "        # === DISTANCE BINS ===\n",
    "        \"bin_starts\": bin_starts,\n",
    "        \"bin_ends\": bin_ends,\n",
    "        \"bin_centers\": bin_centers,\n",
    "        \"n_bins\": len(bin_starts),\n",
    "        # === RING AREAS (for reference) ===\n",
    "        \"ring_areas_nm2\": np.pi * (bin_ends**2 - bin_starts**2),\n",
    "        \"ring_areas_pxl2\": np.pi * (bin_ends**2 - bin_starts**2) / (nm_per_pxl**2),\n",
    "        # === FILE INFORMATION ===\n",
    "        \"ch1_file\": ch1_file,  # Interest points file\n",
    "        \"ch2_file\": ch2_file,  # Reference points file\n",
    "        \"prefix\": prefix,\n",
    "        # === PROCESSING METADATA ===\n",
    "        \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"normalization_method\": \"long_distance_normalization\",\n",
    "        \"normalization_bins_used\": 10,\n",
    "        \"max_correlation_distance_nm\": 800,\n",
    "        # === DATA STRUCTURE INFO ===\n",
    "        \"data_description\": {\n",
    "            \"normalized_results\": \"List of dictionaries, one per cell\",\n",
    "            \"key_fields_per_cell\": [\n",
    "                \"cell_name\",\n",
    "                \"pcf_normalized\",  # Main normalized PCF data\n",
    "                \"pcf_raw\",  # Raw PCF before normalization\n",
    "                \"normalization_factor\",\n",
    "                \"baseline_cv\",\n",
    "                \"quality_assessment\",\n",
    "                \"N_ref\",  # Number of reference points\n",
    "                \"N_interest\",  # Number of interest points\n",
    "                \"rho_ref_per_nm2\",  # Reference point density\n",
    "                \"rho_interest_per_nm2\",  # Interest point density\n",
    "                \"cell_area_nm2\",  # Cell area\n",
    "            ],\n",
    "            \"distance_bins\": \"Use bin_centers for plotting x-axis\",\n",
    "            \"pcf_interpretation\": \"PCF > 1: clustering, PCF = 1: random, PCF < 1: dispersion\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return pcf_data_package\n",
    "\n",
    "\n",
    "# Generate filename and save\n",
    "prefix = ch1_file.split(\"cropped\")[0]\n",
    "fname_save = prefix + \"perCell_crossPCF.p\"\n",
    "\n",
    "print(f\"Preparing to save PCF results...\")\n",
    "print(f\"Prefix: {prefix}\")\n",
    "print(f\"Filename: {fname_save}\")\n",
    "\n",
    "if \"normalized_results\" in locals() and len(normalized_results) > 0:\n",
    "\n",
    "    # Create comprehensive data package\n",
    "    pcf_data_package = save_pcf_results(\n",
    "        normalized_results,\n",
    "        bin_starts,\n",
    "        bin_ends,\n",
    "        nm_per_pxl,\n",
    "        r_max_nm,\n",
    "        ringwidth_nm,\n",
    "        dr_slidingrings_nm,\n",
    "        ch1_file,\n",
    "        ch2_file,\n",
    "        prefix,\n",
    "    )\n",
    "\n",
    "    # Save to pickle file\n",
    "    try:\n",
    "        with open(fname_save, \"wb\") as f:\n",
    "            pickle.dump(pcf_data_package, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print(f\"‚úÖ Successfully saved PCF results to: {fname_save}\")\n",
    "        print(f\"   ‚Ä¢ {pcf_data_package['n_cells']} cells\")\n",
    "        print(f\"   ‚Ä¢ {pcf_data_package['n_bins']} distance bins\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Distance range: {pcf_data_package['bin_centers'][0]:.0f}-{pcf_data_package['bin_centers'][-1]:.0f} nm\"\n",
    "        )\n",
    "        print(f\"   ‚Ä¢ File size: {os.path.getsize(fname_save) / 1024:.1f} KB\")\n",
    "\n",
    "        # Show what's included\n",
    "        print(f\"\\nüì¶ Data package includes:\")\n",
    "        print(f\"   ‚Ä¢ Normalized PCF results for each cell\")\n",
    "        print(f\"   ‚Ä¢ Raw (unnormalized) PCF data\")\n",
    "        print(f\"   ‚Ä¢ Quality assessments and normalization factors\")\n",
    "        print(f\"   ‚Ä¢ Complete spatial parameters and bin definitions\")\n",
    "        print(f\"   ‚Ä¢ Cell metadata (point counts, densities, areas)\")\n",
    "        print(f\"   ‚Ä¢ Processing metadata and timestamps\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving file: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No normalized results found to save\")\n",
    "\n",
    "print(f\"\\nüí° To load later:\")\n",
    "print(f\"   with open('{fname_save}', 'rb') as f:\")\n",
    "print(f\"       loaded_data = pickle.load(f)\")\n",
    "print(f\"   pcf_results = loaded_data['normalized_results']\")\n",
    "print(f\"   bin_centers = loaded_data['bin_centers']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved pickle file\n",
    "prefix = ch1_file.split(\"cropped\")[0]\n",
    "fname_save = prefix + \"perCell_crossPCF.p\"\n",
    "\n",
    "print(f\"Loading saved PCF data from: {fname_save}\")\n",
    "\n",
    "try:\n",
    "    with open(fname_save, \"rb\") as f:\n",
    "        loaded_pcf_data = pickle.load(f)\n",
    "\n",
    "    # Extract the data\n",
    "    loaded_results = loaded_pcf_data[\"normalized_results\"]\n",
    "    bins = loaded_pcf_data[\"bin_centers\"]  # This is what your code expects as 'bins'\n",
    "\n",
    "    print(f\"‚úÖ Successfully loaded:\")\n",
    "    print(f\"   ‚Ä¢ {len(loaded_results)} cells\")\n",
    "    print(f\"   ‚Ä¢ {len(bins)} distance bins\")\n",
    "    print(f\"   ‚Ä¢ Distance range: {bins[0]:.0f}-{bins[-1]:.0f} nm\")\n",
    "\n",
    "    # Verify data integrity\n",
    "    print(f\"\\nüîç Data verification:\")\n",
    "    high_quality_cells = sum(\n",
    "        1 for result in loaded_results if result.get(\"baseline_cv\", 1) <= 0.1\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ High quality cells (CV ‚â§ 10%): {high_quality_cells}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ All cells have normalized PCF: {all('pcf_normalized' in r for r in loaded_results)}\"\n",
    "    )\n",
    "\n",
    "    # Use your plotting code with loaded data\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "    plotted_cells = 0\n",
    "    for result in loaded_results:  # Use loaded_results instead of normalized_results\n",
    "        if result[\"baseline_cv\"] > 0.1:\n",
    "            continue\n",
    "        plt.plot(bins, result[\"pcf_normalized\"], lw=3, label=result[\"cell_name\"])\n",
    "        plotted_cells += 1\n",
    "\n",
    "    plt.axhline(1, ls=\"--\", c=\"gray\", lw=1)\n",
    "\n",
    "    plt.legend(frameon=False, fontsize=11)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    # plt.ylim(0, 5)\n",
    "    plt.xlabel(\"Distance, nm\", fontsize=11)\n",
    "    plt.ylabel(\"PCF\", fontsize=11)\n",
    "    plt.gca().spines[:].set_linewidth(1)\n",
    "    plt.gca().tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"major\",\n",
    "        labelsize=11,\n",
    "        direction=\"in\",\n",
    "        bottom=True,\n",
    "        left=True,\n",
    "        length=5,\n",
    "        width=1,\n",
    "    )\n",
    "    plt.gca().set_axisbelow(False)\n",
    "\n",
    "    plt.savefig(prefix + \"perCell_crossPCF.png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nüìä Plot generated successfully:\")\n",
    "    print(f\"   ‚Ä¢ Plotted {plotted_cells} high-quality cells (baseline CV ‚â§ 10%)\")\n",
    "    print(f\"   ‚Ä¢ Saved as: {prefix}crossPCF.png\")\n",
    "    print(f\"   ‚Ä¢ Distance range shown: 0-1000 nm\")\n",
    "\n",
    "    # Additional verification - compare with original data if available\n",
    "    if \"normalized_results\" in locals():\n",
    "        print(f\"\\nüîÑ Cross-verification with original data:\")\n",
    "        original_high_quality = sum(\n",
    "            1 for result in normalized_results if result.get(\"baseline_cv\", 1) <= 0.1\n",
    "        )\n",
    "        print(f\"   ‚Ä¢ Original high-quality cells: {original_high_quality}\")\n",
    "        print(f\"   ‚Ä¢ Loaded high-quality cells: {high_quality_cells}\")\n",
    "\n",
    "        if original_high_quality == high_quality_cells:\n",
    "            print(f\"   ‚úÖ Cell counts match perfectly!\")\n",
    "\n",
    "            # Check if first cell data matches\n",
    "            if len(loaded_results) > 0 and len(normalized_results) > 0:\n",
    "                orig_pcf = normalized_results[0][\"pcf_normalized\"]\n",
    "                loaded_pcf = loaded_results[0][\"pcf_normalized\"]\n",
    "                if np.allclose(orig_pcf, loaded_pcf):\n",
    "                    print(f\"   ‚úÖ PCF data matches perfectly!\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  PCF data differs - check data integrity\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Cell counts differ - investigate discrepancy\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {fname_save}\")\n",
    "    print(\"   Make sure the pickle file was saved successfully first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading or plotting: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
